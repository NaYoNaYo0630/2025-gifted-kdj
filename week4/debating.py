import uuid
import streamlit as st
import ollama

from utils import check_ollama

st.set_page_config(page_title="AI Debate Room")
st.sidebar.title("Settings")

# ëª¨ë¸ ì„¤ì •
if "model" not in st.session_state:
    check_ollama()
    st.session_state.model = ""
models = [m["model"] for m in ollama.list()["models"]]
st.session_state.model = st.sidebar.selectbox("Choose model", models)

system_prompt = st.sidebar.text_area("System Prompt")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chats" not in st.session_state:
    st.session_state.chats = {}
if "current_chat_id" not in st.session_state:
    st.session_state.current_chat_id = None

# ìƒˆ ì±„íŒ… ë§Œë“¤ê¸°
if st.sidebar.button("\u2795 New Chat"):
    chat_id = str(uuid.uuid4())
    st.session_state.current_chat_id = chat_id
    st.session_state.chats[chat_id] = {
        "name": "ìƒˆë¡œìš´ ì±„íŒ…",
        "messages": [{"role": "system", "content": system_prompt}]
    }
AI1_setting = st.sidebar.text_area(
        "AI1 ì„¤ì •",
        help="ì´ ì„¤ì •ì€ AI1ì˜ ì£¼ì¥ ê²½í–¥, ê°€ì¹˜ê´€ì„ ì¡°ì ˆí•©ë‹ˆë‹¤."
    )
AI2_setting= st.sidebar.text_area(
        "A12 ì„¤ì •",
        value="ì´ ì„¤ì •ì€ AI2ì˜ ì£¼ì¥ ê²½í–¥, ê°€ì¹˜ê´€ì„ ì¡°ì ˆí•©ë‹ˆë‹¤."
    )


# ì±„íŒ… ì„ íƒ
for cid, chat_info in st.session_state.chats.items():
    if st.sidebar.button(chat_info["name"], key=cid):
        st.session_state.current_chat_id = cid

# ì±„íŒ… í™œì„±í™”
chat_id = st.session_state.current_chat_id
if chat_id:
    chat = st.session_state.chats[chat_id]
    st.title(chat["name"])

    for msg in chat["messages"]:
        if msg["role"] != "system":
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    user_input = st.chat_input("Enter debate topic or reply")
    if user_input:
        prompt = f"{user_input}. ì´ê±°ì— ëŒ€í•´ì„œ ë”± ì˜¤ì§ í•˜ë‚˜ì˜ ì˜ê²¬ë§Œ ë‚´ì¤˜"
        chat["messages"].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ğŸ” ì œëª© ìƒì„±: system + user ë©”ì‹œì§€ 2ê°œì¼ ë•Œë§Œ ì‹¤í–‰
        if len(chat["messages"]) == 2 and chat["name"] == "ìƒˆë¡œìš´ ì±„íŒ…" and not chat.get("summarized", False):
            summary_prompt = [
                {"role": "system", "content": "You are a helpful assistant that creates concise chat titles."},
                {"role": "user", "content": f"Generate a short (max 5 words) title summarizing the conversation:\nUser: {chat['messages'][1]['content']}"}
            ]
            try:
                new_title = ""
                stream = ollama.chat(
                    model=st.session_state.model,
                    messages=summary_prompt,
                    stream=True,
                )
                for chunk in stream:
                    new_title += chunk.message.content
                chat["name"] = new_title.strip()
            except:
                pass
            st.rerun()

    responses = []  # ê° AIì˜ ì‘ë‹µì„ ëª¨ì•„ë‘ 

    for i in range(4):
        if i % 2 == 0:
            response = ""
            ai_role = "AI1"
            with st.chat_message(ai_role):
                st.markdown(f"__{ai_role}__")
                placeholder = st.empty()
                
                if i == 0:
                    messages = chat["messages"]
                else:
                    messages = (
                            [{"role": "system", "content": AI1_setting}] +
                            chat["messages"] +
                            [{"role": "assistant", "content": r} for r in responses]
                    )

                stream = ollama.chat(
                    model=st.session_state.model,
                    messages=messages,
                    stream=True
                )
                for chunk in stream:
                    response += chunk.message.content
                    placeholder.markdown(response)
        chat["messages"].append({'role': 'user', 'content': str([response + " Conterpart argue like this. Please rebut and refute this and make your own argue about subject"])})
        if i % 2 == 1:
            ai_role = "AI2"
            response = ""
            with st.chat_message(ai_role):
                st.markdown(f"__{ai_role}__")
                placeholder = st.empty()
                
                if i == 0:
                    messages = chat["messages"]
                else:
                    messages = (
                                [{"role": "system", "content":AI2_setting}] +
                                chat["messages"] +
                                [{"role": "assistant", "content": r} for r in responses]
                            )


                stream = ollama.chat(
                    model=st.session_state.model,
                    messages=messages,
                    stream=True
                )
                for chunk in stream:
                    response += chunk.message.content
                    placeholder.markdown(response)
        chat["messages"].append({'role': 'user', 'content': str([response + " Conterpart argue like this, Please rebut and refute this and make your own argue about subject"])})
        responses.append(response)
        chat["messages"].append({"role": ai_role, "content": response})    
    else:
        st.info("ì™¼ìª½ì—ì„œ ì±„íŒ…ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆ ì±„íŒ…ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
