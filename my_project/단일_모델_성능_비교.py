import os
import re
import json
import time
import random
import pandas as pd
import streamlit as st
import ollama
from utils import check_ollama

# ============================== ìƒìˆ˜ ==============================
LETTERS = ["A", "B", "C", "D"]

# ============================== ìœ í‹¸ ==============================
def clean_surrogates(text: str) -> str:
    return re.sub(r'[\ud800-\udfff]', '', text or "")

@st.cache_data(show_spinner=False)
def load_questions() -> dict:
    """
    ì§€ì› í˜•íƒœ
      - í‰íƒ„: {qid: {question, choices, answer, ...}}
      - ì¤‘ì²©: {domain: {qid: {question, choices, answer, ...}}}
    ë°˜í™˜: í‰íƒ„ dict (qid -> item)
    """
    candidates = [
        "pages/mmlu_debate_questions.json",
        "mmlu_debate_questions.json",
        r"C:\Users\USER\ollama\pages\mmlu_debate_questions.json",
    ]
    for p in candidates:
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                raw = json.load(f)

            def looks_like_question(d: dict) -> bool:
                return isinstance(d, dict) and {"question", "choices", "answer"}.issubset(d.keys())

            is_nested = all(isinstance(v, dict) for v in raw.values()) and not any(
                looks_like_question(v) for v in raw.values()
            )

            if not is_nested:
                # í‰íƒ„ êµ¬ì¡° ê·¸ëŒ€ë¡œ ë°˜í™˜
                return raw

            # ì¤‘ì²© â†’ í‰íƒ„í™”
            flat = {}
            for domain, items in raw.items():
                if not isinstance(items, dict):
                    continue
                for qid, item in items.items():
                    if not looks_like_question(item):
                        continue
                    if not item.get("topic"):
                        item["topic"] = domain
                    flat[qid] = item
            return flat

    st.error("mmlu_debate_questions.json íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

def normalize_item_in_place(q: dict):
    """í‚¤/íƒ€ì… ë³´ì •: promptâ†’question, answersâ†’choices, goldâ†’answer, choices ì¬ë§¤í•‘, topic ì±„ìš°ê¸°"""
    if "question" not in q and "prompt" in q:
        q["question"] = q["prompt"]
    if "choices" not in q and "answers" in q:
        q["choices"] = q["answers"]
    if "answer" not in q and "gold" in q:
        q["answer"] = q["gold"]

    # choices ë³´ì •
    if isinstance(q.get("choices"), list):
        q["choices"] = {LETTERS[i]: q["choices"][i] for i in range(min(4, len(q["choices"])))}
    elif isinstance(q.get("choices"), dict):
        ks = list(q["choices"].keys())
        if any(k not in LETTERS for k in ks):
            vals = list(q["choices"].values())
            q["choices"] = {LETTERS[i]: vals[i] for i in range(min(4, len(vals)))}

    if not q.get("topic"):
        q["topic"] = "(ë¯¸ë¶„ë¥˜)"

def normalize_all_in_place(mmlu_data: dict):
    for _qid, _q in list(mmlu_data.items()):
        if isinstance(_q, dict):
            normalize_item_in_place(_q)

def validate_item_or_stop(qid: str, q: dict):
    need = ["question", "choices", "answer"]
    miss = [k for k in need if k not in q]
    if miss:
        st.error(f"[{qid}] ë¬¸í•­ í‚¤ ëˆ„ë½: {miss}")
        st.json(q)
        st.stop()
    if not isinstance(q["choices"], dict):
        st.error(f"[{qid}] choicesê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤.")
        st.json(q)
        st.stop()
    # ì„ íƒì§€ëŠ” A/B/C/Dë§Œ ìœ ì§€
    for k in list(q["choices"].keys()):
        if k not in LETTERS:
            del q["choices"][k]
    if len(q["choices"]) < 2:
        st.error(f"[{qid}] ì„ íƒì§€ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        st.json(q)
        st.stop()

def extract_choice(text: str) -> str:
    """ëª¨ë¸ ì¶œë ¥ì—ì„œ A/B/C/D 1ê¸€ì ì¶”ì¶œ(ê°•í™” ë²„ì „)"""
    t = (text or "").strip()
    # 1) íƒœê·¸ ìš°ì„ 
    m = re.search(r"<answer>\s*([ABCD])\s*</answer>", t, re.I)
    if m:
        return m.group(1).upper()
    # 2) 'ì •ë‹µ/answer/final is: X'
    m = re.search(r"(ì •ë‹µ|answer|final)\s*(is|:|=)?\s*([ABCD])\b", t, re.I)
    if m:
        return m.group(3).upper()
    # 3) ì¤„ ì „ì²´ê°€ í•œ ê¸€ì
    for line in t.splitlines():
        m = re.match(r"^\s*[\(\[\{<]?\s*([ABCD])\s*[\)\]\}>]?\s*$", line.strip(), re.I)
        if m:
            return m.group(1).upper()
    # 4) ë§ˆì§€ë§‰ fallback
    m = re.findall(r"(?<![A-Z])([ABCD])(?![A-Z])", t.upper())
    return m[0] if m else ""

def build_single_system_prompt() -> str:
    return (
        "ë„ˆëŠ” ê°ê´€ì‹ ì±„ì  ëŒ€ìƒ ë¬¸ì œì˜ ì •ë‹µì„ ê³ ë¥´ëŠ” ëª¨ë¸ì´ë‹¤.\n"
        "ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ `<answer>X</answer>` í•œ ì¤„ë§Œ ì¶œë ¥í•˜ë¼. (XëŠ” A/B/C/D ì¤‘ í•˜ë‚˜)\n"
        "ì„¤ëª…/ë²ˆì—­/ì¶”ê°€ í…ìŠ¤íŠ¸/ê°œí–‰/ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€. ì˜¤ì§ íƒœê·¸ 1ì¤„ë§Œ."
    )

def build_single_user_prompt(q: dict) -> str:
    choices_str = "\n".join([f"{k}: {v}" for k, v in q["choices"].items()])
    return (
        f"ë¬¸ì œ:\n{q['question']}\n\nì„ íƒì§€:\n{choices_str}\n\n"
        "ì§€ì‹œ: ê°€ì¥ ì˜³ì€ í•œ ê°€ì§€ ì„ íƒì§€ë¥¼ ê³ ë¥´ê³  ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ë§Œ ë”°ë¥´ë¼."
    )

def ollama_options(temperature: float, top_p: float, seed: int | None):
    opt = {
        "temperature": float(temperature),
        "top_p": float(top_p),
    }
    if seed is not None:
        opt["seed"] = int(seed)
    return opt

def chat_once(model: str, messages: list, **options) -> str:
    res = ollama.chat(
        model=model,
        messages=messages,
        stream=False,
        options=options or {},
    )
    return clean_surrogates(res.get("message", {}).get("content", ""))

def predict_one(qid: str, q: dict, model: str,
                temperature: float, top_p: float,
                seed: int | None = None, retry: bool = True):
    """ë‹¨ì¼ ëª¨ë¸ì´ ì§ì ‘ A/B/C/Dë¥¼ ê³ ë¦„ -> ê²°ê³¼ dict"""
    normalize_item_in_place(q)
    validate_item_or_stop(qid, q)

    sys = build_single_system_prompt()
    usr = build_single_user_prompt(q)
    try:
        raw = chat_once(
            model,
            [{"role": "system", "content": sys}, {"role": "user", "content": usr}],
            **ollama_options(temperature, top_p, seed),
        )
        pred = extract_choice(raw)
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„(ì§€ì‹œ ê°•í™” + ë³´ìˆ˜ ì„¸íŒ…)
        if not pred and retry:
            raw2 = chat_once(
                model,
                [{"role": "system", "content": sys + "\nì˜ˆ: <answer>C</answer> í˜•ì‹ë§Œ í—ˆìš©."},
                 {"role": "user", "content": usr}],
                **ollama_options(temperature=0.0, top_p=1.0, seed=seed),
            )
            p2 = extract_choice(raw2)
            if p2:
                raw, pred = raw2, p2
    except Exception as e:
        raw, pred = f"[ì˜¤ë¥˜] ëª¨ë¸ ì‹¤íŒ¨: {e}", ""

    correct = (pred == q["answer"])
    return {
        "qid": qid,
        "topic": q.get("topic", ""),
        "question": q["question"],
        "gold": q["answer"],
        "pred": pred or "",
        "correct": bool(correct) if pred else False,
        "raw": raw,
    }

# ---------- í† í”½(ë¬¸ì œ ìœ í˜•) í•„í„° ----------
def topic_of(q: dict) -> str:
    t = (q.get("topic") or "").strip()
    return t if t else "(ë¯¸ë¶„ë¥˜)"

def list_topics(mmlu_data: dict) -> list[str]:
    return sorted({topic_of(q) for q in mmlu_data.values()})

def filter_qids_by_topic(mmlu_data: dict, qids: list[str], picked_topics: list[str]) -> list[str]:
    picked = set(picked_topics)
    return [qid for qid in qids if topic_of(mmlu_data[qid]) in picked]

# ============================== ì•± ==============================
st.set_page_config(page_title="MMLU Single-Model Evaluation", layout="wide")
st.sidebar.title("ğŸ§  MMLU ë‹¨ì¼ ëª¨ë¸ í‰ê°€")

# ë°ì´í„° ë¡œë“œ â†’ ì •ê·œí™”
mmlu_data = load_questions()
normalize_all_in_place(mmlu_data)
all_qids = list(mmlu_data.keys())

if not all_qids:
    st.error("ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤. JSON íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# í† í”½(ë¬¸ì œ ìœ í˜•) í•„í„°
st.sidebar.markdown("### ğŸ—‚ ë¬¸ì œ ìœ í˜•(í† í”½) í•„í„°")
all_topics = list_topics(mmlu_data)
picked_topics = st.sidebar.multiselect("ìœ í˜• ì„ íƒ(ë³µìˆ˜ ê°€ëŠ¥)", all_topics, default=all_topics)

# Ollama ì¤€ë¹„
check_ollama()
try:
    model_list = [m["model"] for m in ollama.list().get("models", [])]
except Exception as e:
    st.error(f"Ollama ëª¨ë¸ ëª©ë¡ ì˜¤ë¥˜: {e}")
    st.stop()
if not model_list:
    st.error("ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. `ollama pull mistral` ë“±ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    st.stop()

# í•˜ì´í¼íŒŒë¼ë¯¸í„° & ëª¨ë¸
st.sidebar.markdown("### âš™ï¸ ì„¤ì •")
temperature = st.sidebar.slider("temperature", 0.0, 1.5, 0.2, 0.1)
top_p = st.sidebar.slider("top_p", 0.1, 1.0, 0.9, 0.05)
use_seed = st.sidebar.checkbox("seed ê³ ì •(ì¬í˜„ì„±)", value=True)
seed = st.sidebar.number_input("seed", value=42, step=1) if use_seed else None
single_model = st.sidebar.selectbox("ì‚¬ìš© ëª¨ë¸(1ê°œ)", model_list, index=0)

mode = st.radio("ì‹¤í–‰ ëª¨ë“œ ì„ íƒ", ["ë‹¨ì¼ ë¬¸ì œ", "ì „ì²´ í‰ê°€"], horizontal=True)

# ------------------------ ë‹¨ì¼ ë¬¸ì œ ------------------------
if mode == "ë‹¨ì¼ ë¬¸ì œ":
    filtered_qids = filter_qids_by_topic(mmlu_data, all_qids, picked_topics)
    if not filtered_qids:
        st.warning("ì„ íƒëœ í† í”½ì— í•´ë‹¹í•˜ëŠ” ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í† í”½ì„ ì¡°ì •í•˜ì„¸ìš”.")
        st.stop()

    # í† í”½ ë¨¼ì € ì„ íƒ â†’ í•´ë‹¹ í† í”½ì—ì„œ ë¬¸ì œ ì„ íƒ
    topics_for_pick = sorted({topic_of(mmlu_data[qid]) for qid in filtered_qids})
    chosen_topic = st.selectbox("ë¬¸ì œ ìœ í˜•(í† í”½)", topics_for_pick)
    topic_qids = [qid for qid in filtered_qids if topic_of(mmlu_data[qid]) == chosen_topic]

    qid = st.selectbox("ë¬¸ì œ ì„ íƒ", topic_qids, key="single_qid")
    q = mmlu_data[qid]

    # í‘œì‹œ ì „ ë³´ì •/ê²€ì¦
    normalize_item_in_place(q)
    validate_item_or_stop(qid, q)

    st.caption(f"ìœ í˜•: {topic_of(q)}")
    st.markdown(f"### â“ {q['question']}")
    for k in LETTERS:
        if k in q["choices"]:
            st.markdown(f"- **{k}**: {q['choices'][k]}")

    if st.button("ğŸš€ ëª¨ë¸ ì˜ˆì¸¡"):
        with st.spinner("ëª¨ë¸ ì˜ˆì¸¡ ì¤‘..."):
            res = predict_one(
                qid=qid, q=q, model=single_model,
                temperature=temperature, top_p=top_p,
                seed=seed, retry=True
            )
        st.markdown(f"**ëª¨ë¸ ì„ íƒ:** {res['pred'] or '(íŒŒì‹± ì‹¤íŒ¨)'}")
        st.markdown(f"**ì •ë‹µ:** {res['gold']}")
        st.markdown("**ì›ë¬¸ ì¶œë ¥(raw):**")
        st.code(res["raw"])
        if res["pred"]:
            if res["correct"]:
                st.success("ì •ë‹µê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤! âœ…")
            else:
                st.error("ì •ë‹µê³¼ ë¶ˆì¼ì¹˜! âŒ")
        else:
            st.warning("ëª¨ë¸ ì¶œë ¥ì—ì„œ A/B/C/D íŒŒì‹± ì‹¤íŒ¨.")

# ------------------------ ì „ì²´ í‰ê°€ ------------------------
else:
    st.markdown("### ğŸ“š ì „ì²´ í‰ê°€ ì„¤ì •")

    filtered_qids = filter_qids_by_topic(mmlu_data, all_qids, picked_topics)
    if not filtered_qids:
        st.warning("ì„ íƒëœ í† í”½ì— í•´ë‹¹í•˜ëŠ” ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í† í”½ì„ ì¡°ì •í•˜ì„¸ìš”.")
        st.stop()

    random_order = st.checkbox("ë¬¸ì œ ìˆœì„œ ì„ê¸°", value=False)
    seed_shuffle = st.number_input("ëœë¤ ì‹œë“œ(ë¬¸í•­ ìˆœì„œ)", value=42, step=1)
    max_count = st.slider("í‰ê°€í•  ë¬¸ì œ ê°œìˆ˜", 1, len(filtered_qids), len(filtered_qids))
    show_logs = st.checkbox("ë¬¸í•­ë³„ ìƒì„¸ ë¡œê·¸ í‘œì‹œ", value=False)

    selected_qids = filtered_qids.copy()
    if random_order:
        random.Random(seed_shuffle).shuffle(selected_qids)
    selected_qids = selected_qids[:max_count]

    if st.button("ğŸ§ª ì „ì²´ í‰ê°€ ì‹œì‘"):
        results = []
        progress = st.progress(0, text="ì‹œì‘ ì¤‘...")
        start_time = time.time()

        for idx, qid in enumerate(selected_qids, start=1):
            q = mmlu_data[qid]
            normalize_item_in_place(q)
            validate_item_or_stop(qid, q)

            progress.progress(idx / max_count, text=f"{idx}/{max_count} í‰ê°€ ì¤‘: {qid}")
            res = predict_one(
                qid=qid, q=q, model=single_model,
                temperature=temperature, top_p=top_p,
                seed=seed, retry=True
            )
            results.append(res)

            if show_logs:
                with st.expander(f"[{idx}] {qid} | ìœ í˜•: {topic_of(q)} | ì •ë‹µ: {res['gold']} | ì˜ˆì¸¡: {res['pred']} | {'âœ…' if res['correct'] else 'âŒ'}"):
                    st.write(q["question"])
                    st.write({k: v for k, v in q["choices"].items()})
                    st.markdown("**ëª¨ë¸ ì›ë¬¸ ì¶œë ¥(raw)**")
                    st.code(res["raw"])

        total_time = time.time() - start_time
        progress.progress(1.0, text="ì™„ë£Œ")

        # ìš”ì•½
        df = pd.DataFrame(results)
        st.markdown("## ğŸ“ˆ ê²°ê³¼ ìš”ì•½")
        if df.empty:
            st.warning("ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            st.stop()

        acc = df["correct"].mean()
        c1, c2, c3 = st.columns(3)
        c1.metric("ì´ ë¬¸í•­", len(df))
        c2.metric("ì •í™•ë„(%)", f"{acc*100:.1f}")
        c3.metric("ì´ ì†Œìš”ì‹œê°„(ì´ˆ)", f"{total_time:.1f}")

        # í† í”½ë³„ ì •í™•ë„ + ë¬¸ì œ ìˆ˜
        st.markdown("### ğŸ§­ ë¬¸ì œ ìœ í˜•ë³„ ì •í™•ë„")
        df["topic_label"] = df["topic"].apply(lambda t: t.strip() if isinstance(t, str) and t.strip() else "(ë¯¸ë¶„ë¥˜)")
        topic_group = df.groupby("topic_label").agg(
            n=("correct", "size"),
            accuracy=("correct", "mean")
        ).reset_index()
        topic_group["accuracy(%)"] = (topic_group["accuracy"] * 100).round(1)
        st.dataframe(topic_group.sort_values(["accuracy(%)", "n"], ascending=[False, False]), use_container_width=True)

        # í˜¼ë™ í–‰ë ¬
        st.markdown("### ğŸ” í˜¼ë™ í–‰ë ¬ (ì˜ˆì¸¡ vs ì •ë‹µ)")
        cm = pd.crosstab(df["gold"], df["pred"], dropna=False).reindex(index=LETTERS, columns=LETTERS, fill_value=0)
        st.dataframe(cm, use_container_width=True)

        # CSV ë‹¤ìš´ë¡œë“œ
        st.markdown("### â¬‡ï¸ ê²°ê³¼ ì €ì¥")
        file_name = f"mmlu_single_model_results_{single_model.replace(':','_')}_{time.strftime('%Y%m%d_%H%M%S')}.csv"
        csv = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", data=csv, file_name=file_name, mime="text/csv")
