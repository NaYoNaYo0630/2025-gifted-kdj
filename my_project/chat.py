import uuid
import streamlit as st
import ollama
from utils import check_ollama

st.set_page_config(page_title="AI Debate Room")
st.sidebar.title("Settings")

# ëª¨ë¸ ì„¤ì •
if "model" not in st.session_state:
    check_ollama()
    st.session_state.model = ""
models = [m["model"] for m in ollama.list()["models"]]
st.session_state.model = st.sidebar.selectbox("Choose model", models)

system_prompt = st.sidebar.text_area("System Prompt")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "chats" not in st.session_state:
    st.session_state.chats = {}
if "current_chat_id" not in st.session_state:
    st.session_state.current_chat_id = None

# ìƒˆ ì±„íŒ… ë§Œë“¤ê¸°
if st.sidebar.button("\u2795 New Chat"):
    chat_id = str(uuid.uuid4())
    st.session_state.current_chat_id = chat_id
    st.session_state.chats[chat_id] = {
        "name": "ìƒˆë¡œìš´ ì±„íŒ…",
        "messages": [{"role": "system", "content": system_prompt}]
    }

AI1_setting = st.sidebar.text_area("AI1 ì„¤ì •", help="AI1ì˜ ì£¼ì¥ ê²½í–¥ ë° ìŠ¤íƒ€ì¼")
AI2_setting = st.sidebar.text_area("AI2 ì„¤ì •", help="AI2ì˜ ì£¼ì¥ ê²½í–¥ ë° ìŠ¤íƒ€ì¼")

# ì±„íŒ… ì„ íƒ
for cid, chat_info in st.session_state.chats.items():
    if st.sidebar.button(chat_info["name"], key=cid):
        st.session_state.current_chat_id = cid

# ì±„íŒ… ì§„í–‰
chat_id = st.session_state.current_chat_id
if chat_id:
    chat = st.session_state.chats[chat_id]
    st.title(chat["name"])

    # ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
    for msg in chat["messages"]:
        if msg["role"] != "system":
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    user_input = st.chat_input("Enter debate topic or reply")

    # ì˜¤ì§ ì‚¬ìš©ìê°€ ì…ë ¥í–ˆì„ ë•Œë§Œ í† ë¡  ì‹œì‘
    if user_input:
        prompt = f"{user_input}\nì´ ì£¼ì œì— ëŒ€í•´ ë”± í•˜ë‚˜ì˜ ì£¼ì¥ë§Œ ë§í•´ì£¼ì„¸ìš”. ì¸ì‚¬ë§ì´ë‚˜ ì„œë¡  ì—†ì´ ë³¸ë¡ ë§Œ."
        chat["messages"].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì œëª© ìš”ì•½
        if len(chat["messages"]) == 2 and chat["name"] == "ìƒˆë¡œìš´ ì±„íŒ…" and not chat.get("summarized", False):
            summary_prompt = [
                {"role": "system", "content": "You are a helpful assistant that creates concise chat titles."},
                {"role": "user", "content": f"Generate a short (max 5 words) title summarizing the conversation:\nUser: {chat['messages'][1]['content']}"}
            ]
            try:
                new_title = ""
                stream = ollama.chat(model=st.session_state.model, messages=summary_prompt, stream=True)
                for chunk in stream:
                    new_title += chunk.message.content
                chat["name"] = new_title.strip()
            except:
                pass
            st.rerun()

        # AI í† ë¡  4í„´ (AI1 â†’ AI2 â†’ AI1 â†’ AI2)
        responses = []

        max_turn = 10
        for i in range(max_turn):
            ai_role = "AI1" if i % 2 == 0 else "AI2"
            setting = AI1_setting if ai_role == "AI1" else AI2_setting
            response = ""

            with st.chat_message(ai_role):
                st.markdown(f"__{ai_role}__")
                placeholder = st.empty()

                # ë©”ì‹œì§€ êµ¬ì„±
                messages = [{"role": "system", "content": setting}] + chat["messages"]

                stream = ollama.chat(model=st.session_state.model, messages=messages, stream=True)
                for chunk in stream:
                    response += chunk.message.content
                    placeholder.markdown(response)

            chat["messages"].append({"role": "assistant", "content": response})
            responses.append(response)

            # ë‹¤ìŒ í„´ìš© ì‚¬ìš©ì ì§€ì‹œ ì‚½ì…
            rebuttal = f"The counterpart just said: {response}\nPlease rebut and make your own argument."
            chat["messages"].append({"role": "user", "content": rebuttal})
            judge_prompt = [
            {"role": "system", "content": "You are a debate observer. Decide if the debate has reached a meaningful conclusion. Reply only with 'stop' or 'continue'."},
            *chat["messages"]
            ]

            judge_response = ""
            stream = ollama.chat(model=st.session_state.model, messages=judge_prompt, stream=True)
            for chunk in stream:
                judge_response += chunk.message.content.lower().strip()

                if "stop" in judge_response:
                    st.success("ğŸ§‘â€âš–ï¸ Judge: This debate has reached a conclusion.")
                    break

else:
    st.info("ì™¼ìª½ì—ì„œ ì±„íŒ…ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆ ì±„íŒ…ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
