

import uuid
import json
import streamlit as st
import ollama
from pages.utils import check_ollama
import re

def clean_surrogates(text):
    return re.sub(r'[\ud800-\udfff]', '', text)

st.set_page_config(page_title="MMLU Debate Evaluation")
st.sidebar.title("ğŸ§  MMLU í† ë¡  í‰ê°€")

# Load MMLU ë¬¸ì œ
with open("mmlu_debate_questions.json", "r", encoding="utf-8") as f:
    mmlu_data = json.load(f)

# ë¬¸ì œ ì„ íƒ
qid = st.sidebar.selectbox("ë¬¸ì œ ì„ íƒ", list(mmlu_data.keys()))
q = mmlu_data[qid]

# ë¬¸ì œ ë° ì„ íƒì§€ ì¶œë ¥
st.markdown(f"### â“ {q['question']}")
for key, val in q['choices'].items():
    st.markdown(f"- **{key}**: {val}")

# ëª¨ë¸ ì„ íƒ
check_ollama()
model_list = [m["model"] for m in ollama.list()["models"]]
models = []
for i in range(4):
    models.append(st.sidebar.selectbox(f"ëª¨ë¸ {i+1} ì„ íƒ", model_list, key=f"model{i+1}"))

# í† ë¡  ì‹¤í–‰
if st.button("ğŸš€ í† ë¡  ì‹œì‘ ë° ì±„ì "):
    st.subheader("ğŸ’¬ AI ë°œì–¸")
    debate_messages = []

    for i in range(4):
        ai_role = f"AI{i+1}"
        system_prompt = f"You are {ai_role}. You are debating to defend the answer choice {chr(65+i)}: {q['choices'][chr(65+i)]}."
        user_msg = q['question'] + "\nChoices:\n" + "\n".join([f"{k}: {v}" for k,v in q['choices'].items()]) + f"\nExplain why {chr(65+i)} is the correct answer."

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_msg}
        ]

        res = ollama.chat(model=models[i], messages=messages, stream=False)
        content = res['message']['content']
        st.markdown(f"**{ai_role} ({chr(65+i)} ì£¼ì¥)**")
        st.info(content)

        debate_messages.append({"role": ai_role, "content": content})

    # Judge ëª¨ë¸ ì‚¬ìš©
    st.subheader("ğŸ§‘â€âš–ï¸ Judge ëª¨ë¸ íŒë‹¨")
    judge_instruction = (
        "ìœ„ í† ë¡  ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜³ì€ ì„ íƒì§€ë¥¼ í•˜ë‚˜ë§Œ ê³¨ë¼ë¼. "
        "ë‹µë³€ì€ ì˜¤ì§ 'A', 'B', 'C', 'D' ì¤‘ í•˜ë‚˜ë§Œ ë‹¨ë‹µìœ¼ë¡œ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆë¼."
    )

    judge_prompt = [
        {"role": "system", "content": judge_instruction},
        *debate_messages
    ]

    judge_res = ollama.chat(model="mistral", messages=judge_prompt, stream=False)
    final_choice = judge_res["message"]["content"].strip().upper()

    st.markdown(f"**ğŸ¯ Judge ì„ íƒ: {final_choice}**")
    st.markdown(f"**âœ… ì •ë‹µ: {q['answer']}**")

    if final_choice == q['answer']:
        st.success("ì •ë‹µê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤! ëª¨ë¸ì˜ íŒë‹¨ì´ ì •í™•í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.error("ì •ë‹µê³¼ ë¶ˆì¼ì¹˜! ëª¨ë¸ íŒë‹¨ì´ í‹€ë ¸ìŠµë‹ˆë‹¤.")
