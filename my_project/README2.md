# 🧠 MMLU Debate Evaluation

여러 AI 모델이 **MMLU(Massive Multitask Language Understanding)** 문제를 두고  
서로 다른 선택지를 옹호하며 **토론 형식**으로 답을 내는 성능 비교 앱입니다.  

- AI들은 **A/B/C/D 선택지** 각각을 맡아 토론  
- Judge 모델은 가장 설득력 있는 답안을 고름  
- 번들 저지(한 모델이 여러 심사위원 연기)로 안정적인 평가 가능  

---

## 🚀 사용 방법

### 1️⃣ 문제 불러오기
- `mmlu_debate_questions.json` 파일을 자동 로드  
- **사이드바 → 🗂 문제 유형(토픽) 필터**에서 원하는 주제만 선택  

---

### 2️⃣ 토론자 & 저지 설정
- **⚡ 1인 다역** : 하나의 모델이 동시에 A/B/C/D를 연기 (빠르고 효율적)  
- **다중 모델** : 각 선택지에 서로 다른 모델 할당  
- **Judge 모델 선택** : 기본 `mistral`, 원하는 다른 모델도 가능  
- **번들 저지** : 한 Judge가 여러 심사위원(J1~J5)을 연기해 투표  

---

### 3️⃣ 공통 설정
- `temperature`, `top_p` : 발언 다양성 조절  
- `발언 문장 수` : 각 발언 길이 제한  
- `num_ctx`, `top_k`, `repeat_penalty` : 세부 하이퍼파라미터  

---

### 4️⃣ 실행 모드
- **단일 문제 모드**  
  - 특정 문제 선택 → 토론 시작 → Judge 결과 확인  
  - Judge 선택 vs 정답 비교, 로그/발언 내용 확인 가능  

- **전체 평가 모드**  
  - 선택된 토픽 전체 문제에 대해 일괄 실행  
  - 정확도, 도메인별/토픽별 성능, 혼동 행렬까지 자동 집계  
  - 결과 CSV 다운로드 가능  

---

## 📊 출력 예시
- 💬 **AI 발언** : 각 선택지별 토론 내용  
- ⚖️ **Judge 모델 판단** : 선택지 + 정답 비교  
- 📈 **결과 요약** : 정확도, 소요 시간, 분야별/주제별 성능  
- 🔁 **혼동 행렬** : Judge 예측 vs 실제 정답  
- ⬇️ **CSV 다운로드** : 분석 결과 저장  

---

👉 이 앱을 통해 **여러 모델의 추론·토론 능력과 정답률**을  
한눈에 비교·분석할 수 있습니다!
